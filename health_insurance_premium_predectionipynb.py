# -*- coding: utf-8 -*-
"""Health_Insurance_Premium_Predectionipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PK3SNhbgMc2CVLQH7fkG0eVUa4edcTm3

#Importing Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split,cross_val_score,learning_curve
from sklearn.metrics import accuracy_score,r2_score,mean_squared_error, mean_absolute_error,confusion_matrix
from sklearn.inspection import permutation_importance

#Reading the data
df=pd.read_csv("/content/Health_insurance.csv")

# Displaying the first 5 rows in the dataset
df.head()

df.shape

"""#Data Pre-Processing"""

df.info()

#Checking whether the dataset contain null value or not
df.isna().sum()

# Description about the data
df.describe()

"""#Exploratory Data-Analysis"""

# Bar plot for Smokers count
df["smoker"].value_counts().plot(kind='bar')
plt.title('Count of smoker')

"""# Detecting and Handling outliers using z-score method"""

#Box plot before HAndling outliers
plt.figure(figsize=(12,5))
sns.boxplot(x='bmi',data=df)

# Detecting outliers
z_scores = np.abs((df['bmi'] - df['bmi'].mean()) / df['bmi'].std())
outliers = df[z_scores > 3]

#Handling outliers
mean_bmi = df['bmi'].mean()
for index, row in outliers.iterrows():
    if row['bmi'] > mean_bmi:
        df.at[index, 'bmi'] = mean_bmi

# Boxplot after handling outliers in BMI
plt.figure(figsize=(12,5))
sns.boxplot(x='bmi',data=df)

#Violin plot for Age
plt.figure(figsize=(12,5))
sns.violinplot(x='age',data=df)

#Boxplot before handling outliers
plt.figure(figsize=(12,5))
sns.boxplot(x='children',data=df)

z_scores_age = np.abs((df['children'] - df['children'].mean()) / df['children'].std())
outliers_age = df[z_scores_age > 3]
median_age = df['children'].median()

for index, row in outliers_age.iterrows():
    if row['children'] > median_age:
        df.at[index, 'children'] = median_age

#Boxplot After handling outliers
plt.figure(figsize=(12,5))
sns.boxplot(x='children',data=df)

df.describe()

#Histogram for Charges
plt.figure(figsize=(15,5))
sns.histplot(x='charges',data=df)

#Boxplot After handling outliers
plt.figure(figsize=(8,4))
sns.violinplot(x="smoker",y="bmi",data=df)

#Scatter plot between Region and Bmi
x=df["region"]
y=df["bmi"]
plt.scatter(x,y)
plt.title("Region vs Bmi")
plt.xlabel("Region")
plt.ylabel("BMI")
plt.show()

#Stripplot for Region vs Charges
sns.stripplot(data=df, x='region', y='charges', hue=df.smoker).set(title='Region vs Charges')

# Line plot for Age vs Charges for people with 3-5 children
sns.lineplot(x=df.age, y=df.charges, hue=df.smoker, data= df).set(title='Age vs Charges for people with 3-5 children')

#Line plot between BMI and CHARGES
sns.lineplot(data=df,x='bmi',y='charges', hue='smoker').set(title='BMI vs charges')

#Visualizing  pair blot based on Smokers
sns.pairplot(data=df,hue='smoker')

"""#Changing categorical values into numerical """

#Binary Encoding
df["smoker"]=df["smoker"].map({"yes":0,"no":1})

df["sex"]=df["sex"].map({"male":0,"female":1})

#Finding all the unique values in region column
df["region"].unique()

df["region"]=df["region"].map({"southwest":0,"southeast":1,"northwest":2,"northeast":3})

df

# Correlation between All the columns
df.corr()

#Heat map 
sns.heatmap(df.corr(), cmap='Wistia', annot=True)

#One Hot Encoding
"""encoder=OneHotEncoder()
Encoded_region=encoder.fit_transform(df[["region"]])
one_hot = pd.DataFrame(Encoded_region.toarray(), columns=encoder.get_feature_names_out(['region']))
dff=df.drop(["region"],axis=1)
data=pd.concat([dff,one_hot],axis=1)"""

#data

#sns.heatmap(data.corr(), cmap='Wistia', annot=True)

"""#Splitting the data using Train Test Split"""

y=df["charges"]

y

x = df.drop("charges",axis=1,inplace=True)

x=df
x

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=50)
x_train.shape,y_train.shape,x_test.shape,y_test.shape

"""#RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor

rf=RandomForestRegressor()

rf.fit(x_train,y_train)

y_pred1=rf.predict(x_test)

y_test

"""#Evaluating Accuracy"""

r2_score(y_test,y_pred1)

mean_squared_error(y_test,y_pred1)

mean_absolute_error(y_test,y_pred1)

cross_val_score(rf,x_train,y_train)

sdf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred1})

sdf

estimator = rf

train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=5)

# plot the learning curve using matplotlib
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, test_mean, label='Cross-validation score')
plt.xlabel('Training set size')
plt.ylabel('Accuracy score')
plt.legend()
plt.show()

plt.scatter(y_test, y_pred1, cmap="coolwarm")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.colorbar()
plt.show()

# estimator = model, X = feature matrix, y = target vector
result = permutation_importance(estimator, x, y, n_repeats=10, random_state=0)

# plot the feature importance using matplotlib
sorted_idx = result.importances_mean.argsort()
plt.barh(range(x.shape[1]), result.importances_mean[sorted_idx])
plt.yticks(range(x.shape[1]), x.columns[sorted_idx])
plt.xlabel('Feature importance')
plt.ylabel('Features')
plt.show()

"""#XGBOOST"""

import xgboost as xgb

xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 1000)

xgb_model.fit(x_train, y_train)

y_pred2=xgb_model.predict(x_test)

r2_score(y_test,y_pred2)

mean_squared_error(y_test,y_pred2)

mean_absolute_error(y_test,y_pred2)

cross_val_score(xgb_model,x_train,y_train)

sdf2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred2})
sdf2

estimator = xgb_model

train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=5)

# plot the learning curve using matplotlib
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, test_mean, label='Cross-validation score')
plt.xlabel('Training set size')
plt.ylabel('Accuracy score')
plt.legend()
plt.show()

result = permutation_importance(estimator, x, y, n_repeats=10, random_state=0)

# plot the feature importance using matplotlib
sorted_idx = result.importances_mean.argsort()
plt.barh(range(x.shape[1]), result.importances_mean[sorted_idx])
plt.yticks(range(x.shape[1]), x.columns[sorted_idx])
plt.xlabel('Feature importance')
plt.ylabel('Features')
plt.show()

plt.scatter(y_test, y_pred2, cmap="coolwarm")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.colorbar()
plt.show()

"""#Gradient Boosting Regressor"""

from sklearn.ensemble import GradientBoostingRegressor

gbr=GradientBoostingRegressor()

gbr.fit(x_train,y_train)

y_pred3=gbr.predict(x_test)

r2_score(y_test,y_pred3)

mean_squared_error(y_test,y_pred3)

mean_absolute_error(y_test,y_pred2)

cross_val_score(gbr,x_train,y_train)

sdf3= pd.DataFrame({'Actual': y_test, 'Predicted': y_pred2})
sdf3

estimator = gbr

train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=5)

# plot the learning curve using matplotlib
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, test_mean, label='Cross-validation score')
plt.xlabel('Training set size')
plt.ylabel('Accuracy score')
plt.legend()
plt.show()

result = permutation_importance(estimator, x, y, n_repeats=10, random_state=0)

# plot the feature importance using matplotlib
sorted_idx = result.importances_mean.argsort()
plt.barh(range(x.shape[1]), result.importances_mean[sorted_idx])
plt.yticks(range(x.shape[1]), x.columns[sorted_idx])
plt.xlabel('Feature importance')
plt.ylabel('Features')
plt.show()

plt.scatter(y_test, y_pred3, cmap="coolwarm")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.colorbar()
plt.show()

"""## Linear Regression algorithm"""

from sklearn.linear_model import LinearRegression

lr=LinearRegression()

lr.fit(x_train,y_train)

y_pred4=lr.predict(x_test)

r2_score(y_test,y_pred4)

mean_squared_error(y_test,y_pred4)

mean_absolute_error(y_test,y_pred4)

cross_val_score(lr,x_train,y_train)

sdf=pd.DataFrame({'Actual': y_test, 'Predicted': y_pred4})
sdf

estimator = lr

train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=5)

# plot the learning curve using matplotlib
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, test_mean, label='Cross-validation score')
plt.xlabel('Training set size')
plt.ylabel('Accuracy score')
plt.legend()
plt.show()

result = permutation_importance(estimator, x, y, n_repeats=10, random_state=0)

# plot the feature importance using matplotlib
sorted_idx = result.importances_mean.argsort()
plt.barh(range(x.shape[1]), result.importances_mean[sorted_idx])
plt.yticks(range(x.shape[1]), x.columns[sorted_idx])
plt.xlabel('Feature importance')
plt.ylabel('Features')
plt.show()

plt.scatter(y_test, y_pred4, cmap="coolwarm")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.colorbar()
plt.show()

"""#Decision Tree Regressor"""

from sklearn.tree import DecisionTreeRegressor

reg=DecisionTreeRegressor(criterion="squared_error",max_depth=10,min_samples_split=10,random_state=5)

reg.fit(x_train,y_train)

y_pred5=reg.predict(x_test)

r2_score(y_test,y_pred5)

mean_squared_error(y_test,y_pred5)

mean_absolute_error(y_test,y_pred5)

cross_val_score(reg,x_train,y_train)

sdf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred5})
sdf

estimator = reg

train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=5)

# plot the learning curve using matplotlib
train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, test_mean, label='Cross-validation score')
plt.xlabel('Training set size')
plt.ylabel('Accuracy score')
plt.legend()
plt.show()

result = permutation_importance(estimator, x, y, n_repeats=10, random_state=0)

# plot the feature importance using matplotlib
sorted_idx = result.importances_mean.argsort()
plt.barh(range(x.shape[1]), result.importances_mean[sorted_idx])
plt.yticks(range(x.shape[1]), x.columns[sorted_idx])
plt.xlabel('Feature importance')
plt.ylabel('Features')
plt.show()

plt.scatter(y_test, y_pred5, cmap="coolwarm")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.colorbar()
plt.show()

"""#Predictive model

"""

age=int(input("Enter age:"))
sex=int(input("Enter sex(MAle:0, Female:1:"))
bmi=float(input("Enter body mass index:"))
Children=int(input("Enter no of children:"))
smoker=int(input("Enter whether the person is smoker or not(Ye:0,No:1::"))
region=int(input("Enter Region({Southwest:0,southeast:1,nortwest:2,northeast:3}:"))
input_data = (age,sex,bmi,Children,smoker,region)
# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)
# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)
# standardize the input data
prediction = gbr.predict(input_data_reshaped)
print("The predicted charges are:",prediction)